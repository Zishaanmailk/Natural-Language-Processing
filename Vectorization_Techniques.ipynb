{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "937d4c36-8d84-481a-9078-c252884c8d60",
   "metadata": {},
   "source": [
    "# Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7747e104-008a-4c94-9231-1fce07c86610",
   "metadata": {},
   "source": [
    "## 1. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51549089-387e-49ce-ab00-8bd5b57d0d8c",
   "metadata": {},
   "source": [
    "One-Hot Encoding is a method that represents each token (word) in a vocabulary as a binary vector whose length equals the vocabulary size.\n",
    "Only one position in the vector is marked with 1, and all other positions are 0.\n",
    "\n",
    "Each word has its own unique vector.\n",
    "\n",
    "When applied to a sentence, One-Hot Encoding produces a matrix, where:\n",
    "\n",
    "Each row = one token from the sentence\n",
    "\n",
    "Each row is a one-hot vector (a unique indicator representation)\n",
    "\n",
    "* `sen1= i love nlp`\n",
    "\n",
    "* `sen2= i dont like nlp`\n",
    "\n",
    "`[ I ]     → [1, 0, 0, 0, 0]`\n",
    "\n",
    "`[ love ]  → [0, 1, 0, 0, 0]`\n",
    "\n",
    "`[ nlp ]   → [0, 0, 1, 0, 0]`\n",
    "\n",
    "after vectorization:\n",
    "\n",
    "`sen1=[[1, 0, 0, 0, 0],`\n",
    "\n",
    "       [0, 1, 0, 0, 0],\n",
    " \n",
    "       [0, 0, 1, 0, 0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e418f12-2568-4138-8e22-2a57e8c8ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b98c18-564b-4644-9c2c-023ac6e652ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences = {\n",
    "    0: [\"i\", \"love\", \"nlp\"],\n",
    "    1: [\"nlp\", \"is\", \"fun\", \"and\", \"powerful\"],\n",
    "    2: [\"i\", \"am\", \"learning\"],\n",
    "    3: [\"deep\", \"learning\", \"and\", \"nlp\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42007cad-04a9-4a1c-995f-0e889e425b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build exploded DataFrame: one row per (sentence_id, token)\n",
    "rows = []\n",
    "for sid, tokens in sentences.items():\n",
    "    for t in tokens:\n",
    "        rows.append({\"sent_id\": sid, \"token\": t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f11385c-4ae1-4a1d-a0a4-2a74386af75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sent_id': 0, 'token': 'i'},\n",
       " {'sent_id': 0, 'token': 'love'},\n",
       " {'sent_id': 0, 'token': 'nlp'},\n",
       " {'sent_id': 1, 'token': 'nlp'},\n",
       " {'sent_id': 1, 'token': 'is'},\n",
       " {'sent_id': 1, 'token': 'fun'},\n",
       " {'sent_id': 1, 'token': 'and'},\n",
       " {'sent_id': 1, 'token': 'powerful'},\n",
       " {'sent_id': 2, 'token': 'i'},\n",
       " {'sent_id': 2, 'token': 'am'},\n",
       " {'sent_id': 2, 'token': 'learning'},\n",
       " {'sent_id': 3, 'token': 'deep'},\n",
       " {'sent_id': 3, 'token': 'learning'},\n",
       " {'sent_id': 3, 'token': 'and'},\n",
       " {'sent_id': 3, 'token': 'nlp'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15fbb589-a87b-42e1-9b75-433d3deac143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>powerful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_id     token\n",
       "0         0         i\n",
       "1         0      love\n",
       "2         0       nlp\n",
       "3         1       nlp\n",
       "4         1        is\n",
       "5         1       fun\n",
       "6         1       and\n",
       "7         1  powerful\n",
       "8         2         i\n",
       "9         2        am\n",
       "10        2  learning\n",
       "11        3      deep\n",
       "12        3  learning\n",
       "13        3       and\n",
       "14        3       nlp"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f6c6e1d-e4fa-424c-883b-398c2a4d7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_matrices_catograc = []\n",
    "\n",
    "for sid in sorted(df['sent_id'].unique()):\n",
    "    mat = df[df['sent_id'] == sid].drop(columns=['sent_id']).values\n",
    "    sentence_matrices_catograc.append(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e839227c-95a7-4bf1-926b-ae0c81ad5c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([['i'],\n",
       "        ['love'],\n",
       "        ['nlp']], dtype=object),\n",
       " array([['nlp'],\n",
       "        ['is'],\n",
       "        ['fun'],\n",
       "        ['and'],\n",
       "        ['powerful']], dtype=object),\n",
       " array([['i'],\n",
       "        ['am'],\n",
       "        ['learning']], dtype=object),\n",
       " array([['deep'],\n",
       "        ['learning'],\n",
       "        ['and'],\n",
       "        ['nlp']], dtype=object)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_matrices_catograc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f28f31b2-0afd-44ec-b862-8ae3b580ed11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>deep</th>\n",
       "      <th>fun</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>nlp</th>\n",
       "      <th>powerful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    am  and  deep  fun  i  is  learning  love  nlp  powerful\n",
       "0    0    0     0    0  1   0         0     0    0         0\n",
       "1    0    0     0    0  0   0         0     1    0         0\n",
       "2    0    0     0    0  0   0         0     0    1         0\n",
       "3    0    0     0    0  0   0         0     0    1         0\n",
       "4    0    0     0    0  0   1         0     0    0         0\n",
       "5    0    0     0    1  0   0         0     0    0         0\n",
       "6    0    1     0    0  0   0         0     0    0         0\n",
       "7    0    0     0    0  0   0         0     0    0         1\n",
       "8    0    0     0    0  1   0         0     0    0         0\n",
       "9    1    0     0    0  0   0         0     0    0         0\n",
       "10   0    0     0    0  0   0         1     0    0         0\n",
       "11   0    0     1    0  0   0         0     0    0         0\n",
       "12   0    0     0    0  0   0         1     0    0         0\n",
       "13   0    1     0    0  0   0         0     0    0         0\n",
       "14   0    0     0    0  0   0         0     0    1         0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dummies for each token row for token\n",
    "dummies = pd.get_dummies(df[\"token\"]).astype(int)\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c28ae79e-573e-4308-a3ad-a93f770a218f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>deep</th>\n",
       "      <th>fun</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>nlp</th>\n",
       "      <th>powerful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_id  am  and  deep  fun  i  is  learning  love  nlp  powerful\n",
       "0         0   0    0     0    0  1   0         0     0    0         0\n",
       "1         0   0    0     0    0  0   0         0     1    0         0\n",
       "2         0   0    0     0    0  0   0         0     0    1         0\n",
       "3         1   0    0     0    0  0   0         0     0    1         0\n",
       "4         1   0    0     0    0  0   1         0     0    0         0\n",
       "5         1   0    0     0    1  0   0         0     0    0         0\n",
       "6         1   0    1     0    0  0   0         0     0    0         0\n",
       "7         1   0    0     0    0  0   0         0     0    0         1\n",
       "8         2   0    0     0    0  1   0         0     0    0         0\n",
       "9         2   1    0     0    0  0   0         0     0    0         0\n",
       "10        2   0    0     0    0  0   0         1     0    0         0\n",
       "11        3   0    0     1    0  0   0         0     0    0         0\n",
       "12        3   0    0     0    0  0   0         1     0    0         0\n",
       "13        3   0    1     0    0  0   0         0     0    0         0\n",
       "14        3   0    0     0    0  0   0         0     0    1         0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_dummies = pd.concat([df[[\"sent_id\"]], dummies], axis=1)\n",
    "df_with_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9529e874-a6c7-4a94-ae54-1fc444b3c7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (columns): ['am', 'and', 'deep', 'fun', 'i', 'is', 'learning', 'love', 'nlp', 'powerful'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary (columns):\", list(dummies.columns), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d32686-3e46-4c4c-b885-2d4c141bceee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([['i'],\n",
       "        ['love'],\n",
       "        ['nlp']], dtype=object),\n",
       " array([['nlp'],\n",
       "        ['is'],\n",
       "        ['fun'],\n",
       "        ['and'],\n",
       "        ['powerful']], dtype=object),\n",
       " array([['i'],\n",
       "        ['am'],\n",
       "        ['learning']], dtype=object),\n",
       " array([['deep'],\n",
       "        ['learning'],\n",
       "        ['and'],\n",
       "        ['nlp']], dtype=object)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_matrices_catograc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa05fce3-9015-4815-acdb-5f2a2e7602fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_matrices = []\n",
    "\n",
    "for sid in sorted(df_with_dummies['sent_id'].unique()):\n",
    "    mat = df_with_dummies[df_with_dummies['sent_id'] == sid].drop(columns=['sent_id']).values\n",
    "    sentence_matrices.append(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "432b0d51-4359-4f65-871d-61117e461e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),\n",
       " array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]),\n",
       " array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27f3e7ac-1406-4fd1-b396-eebfcbf6caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1ec3b31-0a1d-4843-a670-e307ce07ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6811fa40-c242-46ac-a011-6e7962448142",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b341a74d-5977-4ca3-9adc-318bf2949f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit OneHotEncoder on all tokens (vocabulary)\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "token_ohe = enc.fit_transform(df[['token']])   # shape: (total_tokens, vocab_size)\n",
    "arr=token_ohe.toarray()\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b2bf342-30d1-477b-97b4-25fd05ec05ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['token_am', 'token_and', 'token_deep', 'token_fun', 'token_i',\n",
       "       'token_is', 'token_learning', 'token_love', 'token_nlp',\n",
       "       'token_powerful'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = enc.get_feature_names_out()  # token names as columns\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa05f836-347c-4feb-967a-7bf847236fe1",
   "metadata": {},
   "source": [
    "## 2. Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77684ac0-c3c3-42a5-8441-8e9e713522c5",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) is a unigram-based text representation technique where each document (sentence or paragraph) is converted into a vector based solely on the frequency of individual words, ignoring grammar, order, and context.\n",
    "In a BoW model, the text is treated as a “bag” of unigrams (single words), where only the presence or count of each word matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "452820d4-1930-4239-a654-7bfeffbf63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bf1294b-e6e4-48e1-818a-6d32db6f3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "root=Path(\"data\")\n",
    "root.mkdir(exist_ok=True)\n",
    "path=root/\"little.csv\"\n",
    "df=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb796a8c-0c99-499b-bf61-6af0721133be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love cats</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I like cats</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cats are good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I dont like cats</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I like dogs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text  output\n",
       "0        I love cats       1\n",
       "1        I like cats       1\n",
       "2     cats are good        1\n",
       "3  I dont like cats        0\n",
       "4        I like dogs       0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "941ea950-9efa-4232-9225-7be7ab1ec9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "# you can set max_features (by default it is false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "528ea6fb-f329-4b01-af4f-88961a289fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow=cv.fit_transform(df['text'])\n",
    "bow=bow.toarray()\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ebe1749-ee73-4d61-b2db-b354dcc345d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6, 'cats': 1, 'like': 5, 'are': 0, 'good': 4, 'dont': 3, 'dogs': 2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5a774e1-6b3e-4fa1-9eb3-e7aff17ba176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fe1e1-feed-47e7-9e80-b0d7125a62e8",
   "metadata": {},
   "source": [
    "## 3. N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c31774-7241-4b6c-9766-0a1b2efdcb11",
   "metadata": {},
   "source": [
    "An n-gram is a contiguous sequence of n tokens (usually words) extracted from a text. N-grams represent local context by preserving the order of tokens within a fixed-length window of size n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dedefe-6771-4c79-984d-80c85189bb7d",
   "metadata": {},
   "source": [
    "`Unigram (n = 1) BoW(bag of words`\n",
    "\n",
    "A unigram is an n-gram consisting of a single token. It treats each word independently without considering surrounding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f10fe8a4-eb79-484b-8958-680552fe6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5db3c0bc-f518-4f34-ac9d-8743c7c4fd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow=cv.fit_transform(df['text'])\n",
    "bow=bow.toarray()\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0df3f5d7-5799-45b6-a1df-919818ca9bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6, 'cats': 1, 'like': 5, 'are': 0, 'good': 4, 'dont': 3, 'dogs': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6810fed5-9410-42a0-8666-fd8d3e4c14ea",
   "metadata": {},
   "source": [
    "`Bigram (n = 2)`\n",
    "\n",
    "A bigram is an n-gram made of two consecutive tokens. It captures the direct relationship between a word and its immediate successor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "549825c5-6894-4ae1-a2be-c0554409b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "905ddd23-5084-4c15-ab72-f08eb22d9d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow=cv.fit_transform(df['text'])\n",
    "bow=bow.toarray()\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41561990-0ab9-4e51-82a1-d5925e500291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love cats': 5,\n",
       " 'like cats': 3,\n",
       " 'cats are': 1,\n",
       " 'are good': 0,\n",
       " 'dont like': 2,\n",
       " 'like dogs': 4}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03b5ea-7ad2-42b3-a2e6-7f4e9c6b8a93",
   "metadata": {},
   "source": [
    "`Trigram (n = 3)`\n",
    "\n",
    "A trigram is an n-gram composed of three consecutive tokens. It preserves a slightly larger context than bigrams by considering two-word dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fa3cba7-7583-41e8-85a3-8f5506398637",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e0e9635-e0af-4853-beee-f6df37e90dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow=cv.fit_transform(df['text'])\n",
    "bow=bow.toarray()\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "360aea24-103b-49f6-8055-f1a382c84c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats are good': 0, 'dont like cats': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8e19f6b-2daf-48f9-8881-25187f18ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d202ac3-1c38-4ef7-93b3-27392efa43a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow=cv.fit_transform(df['text'])\n",
    "bow=bow.toarray()\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5661224-cee3-4b65-9e2d-7fcd25d5ced9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 13,\n",
       " 'cats': 2,\n",
       " 'love cats': 14,\n",
       " 'like': 10,\n",
       " 'like cats': 11,\n",
       " 'are': 0,\n",
       " 'good': 9,\n",
       " 'cats are': 3,\n",
       " 'are good': 1,\n",
       " 'cats are good': 4,\n",
       " 'dont': 6,\n",
       " 'dont like': 7,\n",
       " 'dont like cats': 8,\n",
       " 'dogs': 5,\n",
       " 'like dogs': 12}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24400a1e-6e0d-4ccf-be3a-12f15debc3f6",
   "metadata": {},
   "source": [
    "## 4. TFIDF (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af26f0a0-3ae8-413d-9496-99318a2cc473",
   "metadata": {},
   "source": [
    "TF-IDF is a statistical weighting measure used to evaluate how important a word is to a document within a collection (corpus).\n",
    "It increases proportionally with the number of times a word appears in a document (TF) but is offset by how frequently the word appears across all documents (IDF), which reduces the weight of common terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14044429-a734-48f1-a3bc-2629e58aba85",
   "metadata": {},
   "source": [
    "Term Frequency (TF)\n",
    "\n",
    "* `TF(t, d) = f(t, d) / |d|`\n",
    "\n",
    "where:\n",
    "\n",
    "f(t, d) = frequency of term t in document d\n",
    "\n",
    "|d| = total number of terms in document d\n",
    "\n",
    "Inverse Document Frequency (IDF)\n",
    "\n",
    "* `IDF(t) = log( N / n(t) )`\n",
    "\n",
    "Smoothed form:\n",
    "\n",
    "* `IDF(t) = log( (N + 1) / (n(t) + 1) ) + 1`\n",
    "\n",
    "where:\n",
    "\n",
    "N = total number of documents\n",
    "\n",
    "n(t) = number of documents containing term t\n",
    "\n",
    "TF-IDF\n",
    "\n",
    "* `TF-IDF(t, d) = TF(t, d) × IDF(t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87a764da-3b70-4ebf-a238-ba211e8b1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c8f92dd-8f44-4c3a-af73-2e3c72128423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.49084524, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.87124678],\n",
       "       [0.        , 0.64374446, 0.        , 0.        , 0.        ,\n",
       "        0.76524053, 0.        ],\n",
       "       [0.65690037, 0.37008621, 0.        , 0.        , 0.65690037,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.42395393, 0.        , 0.75251519, 0.        ,\n",
       "        0.5039682 , 0.        ],\n",
       "       [0.        , 0.        , 0.83088075, 0.        , 0.        ,\n",
       "        0.55645052, 0.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit_transform(df['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38a29b23-d26e-498c-ad8b-513eb31ea870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09861229, 1.18232156, 2.09861229, 2.09861229, 2.09861229,\n",
       "       1.40546511, 2.09861229])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3189e05-f033-4401-9595-91a60981ff7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['are', 'cats', 'dogs', 'dont', 'good', 'like', 'love'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83b390-250a-4ab2-b440-5138315a91ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
